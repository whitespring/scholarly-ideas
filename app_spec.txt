<project_specification>
  <project_name>Scholarly Ideas</project_name>

  <overview>
    A web application that helps Management researchers develop rigorous, genuine research puzzles grounded in real empirical anomalies. The app combats common pitfalls in academic research ideation—"literature has overlooked," "let's open the black box," and pure "gap-spotting"—by guiding researchers through a disciplined process of puzzle discovery and articulation. Through a hybrid approach combining Socratic dialogue, diagnostic assessment, and generative suggestions, users leave with clearly articulated puzzles, journal-ready framings, and research direction clarity.
  </overview>

  <core_philosophy>
    Good research starts with genuine puzzles—empirical patterns that contradict or cannot be explained by existing theory. The app embeds this philosophy (drawing on Zuckerman's framework) as background logic without explicitly teaching genre frameworks to users unless they opt into "Teach Me" mode.
  </core_philosophy>

  <technology_stack>
    <frontend>
      <framework>React with TypeScript</framework>
      <styling>Tailwind CSS</styling>
      <component_library>shadcn/ui</component_library>
      <state_management>React Context + useReducer for session state</state_management>
    </frontend>
    <backend>
      <runtime>Next.js API routes (Node.js)</runtime>
      <analysis_service>Python FastAPI microservice for statistical analysis</analysis_service>
      <database>None - export-focused persistence (client-side session state)</database>
    </backend>
    <llm_integration>
      <primary>Claude API (Anthropic) - strong reasoning, large context window</primary>
      <cost_optimization>Claude Haiku for initial screening, Sonnet for deep analysis</cost_optimization>
    </llm_integration>
    <external_apis>
      <literature>Semantic Scholar API (free tier: 100 requests/second)</literature>
      <backup>OpenAlex as supplement if needed</backup>
    </external_apis>
    <deployment>
      <frontend>Vercel (Next.js frontend + API routes)</frontend>
      <python_service>Railway or Render</python_service>
    </deployment>
  </technology_stack>

  <prerequisites>
    <environment_setup>
      - Node.js 18+ for Next.js frontend
      - Python 3.10+ for FastAPI analysis service
      - Claude API key (Anthropic)
      - Semantic Scholar API access (free)
    </environment_setup>
  </prerequisites>

  <feature_count>175</feature_count>

  <security_and_access_control>
    <user_roles>
      <role name="anonymous_user">
        <permissions>
          - Full access to all app features
          - Upload and analyze data (processed in memory only)
          - Export/import sessions
          - No data persisted server-side
        </permissions>
      </role>
    </user_roles>
    <authentication>
      <method>None required - no user accounts</method>
      <session_timeout>Browser session only</session_timeout>
    </authentication>
    <data_handling>
      - All uploads processed in memory, never persisted server-side
      - No user data stored or logged
      - User data never used for model training
      - HTTPS only for all traffic
      - API keys server-side only
      - File validation and malicious content scanning
      - Rate limiting on LLM API calls
      - Optional encryption for exported session files
    </data_handling>
    <institutional_compliance>
      - App processes data transiently; researchers remain responsible for their data
      - No server-side logs of research content
      - Can be deployed on-premises if institution requires
    </institutional_compliance>
  </security_and_access_control>

  <core_features>
    <entry_modes>
      <i_have_an_idea>
        - User describes research direction
        - App evaluates against puzzle-quality criteria
        - Guided refinement through Socratic dialogue
        - Pseudo-puzzle detection and redirection
      </i_have_an_idea>
      <i_have_data>
        - Context gathering: "What is this data, and why did you collect it?"
        - File upload with format detection
        - Variable summaries (quantitative) or theme overview (qualitative)
        - Neutral anomaly presentation: "What stands out to you?"
        - Pattern exploration with rigor warnings
        - Transition to puzzle articulation
      </i_have_data>
      <im_exploring>
        - Broad discovery questions: "What's been on your mind? What surprised you lately?"
        - Literature surfacing as themes emerge
        - Query Semantic Scholar for debates and related papers
        - Guide toward puzzle discovery
      </im_exploring>
    </entry_modes>

    <conversation_interface>
      - Phased conversation flow (Opening → Probing → Literature/Diagnosis → Articulation → Output)
      - Message history with role attribution
      - Feedback escalation tracking (gentle → moderate → direct)
      - "Be more direct" user toggle to skip gentle phases
      - "Teach Me" mode toggle for explicit educational content
      - Suggested action buttons (run analysis, search literature, generate output)
      - Subfield selection (Strategy, OB, Entrepreneurship, etc.)
    </conversation_interface>

    <pseudo_puzzle_detection>
      <patterns_detected>
        - "Literature has overlooked X" → Probe for what's genuinely puzzling
        - "Let's open the black box" → Challenge whether process detail adds explanatory power
        - "Gap in literature" → Demand real-world anomaly, not just unstudied topic
        - Data dredging → Multiple testing warnings
      </patterns_detected>
      <feedback_approach>
        <default_off>
          - Implicit feedback through probing questions
          - Graduated escalation based on persistence
          - Redirect to adjacent areas where genuine puzzles might live
        </default_off>
        <teach_me_mode>
          - First: Name the pattern ("This sounds like gap-spotting—here's why that's risky")
          - Then: Offer "want to learn more?" for deeper explanations with real paper examples
        </teach_me_mode>
      </feedback_approach>
    </pseudo_puzzle_detection>

    <data_analysis>
      <supported_formats>
        - CSV and Excel files
        - Stata (.dta)
        - SPSS (.sav)
        - R data files (.rds, .rda)
        - Qualitative text (transcripts, field notes as TXT)
        - PDF documents (draft papers, notes)
      </supported_formats>
      <size_limit>Under 10MB (hundreds to thousands of rows)</size_limit>
      <quantitative_analysis>
        - Descriptive summary: Variables, basic statistics
        - Pattern detection: Statistical outliers, surprising subgroups
        - Anomaly detection: Theory violations
        - Theory matching: Identify which theories data could speak to (derived from literature)
      </quantitative_analysis>
      <qualitative_analysis>
        - Theme detection: Recurring themes and frequency across documents
        - Surprising quote surfacing: Quotes contradicting common assumptions or theories
        - Summarization: Structured summary for LLM reference
      </qualitative_analysis>
      <rigor_safeguards>
        - Multiple testing warnings when findings might be chance
        - Theory-first prompting before showing anomalies
        - Replication emphasis—validate in holdout/new data
        - Neutral presentation: "Here's what the data shows" (let user interpret)
      </rigor_safeguards>
      <execution_model>
        - Basic stats run automatically on upload
        - Deeper analysis on user request
      </execution_model>
    </data_analysis>

    <literature_integration>
      <api>Semantic Scholar API</api>
      <capabilities>
        - Novelty check: Surface 3-5 most similar papers, ask "how is your angle different?"
        - Theory extraction: Identify relevant theories and predictions dynamically from search
        - Conversation mapping: Show which scholarly debates the puzzle speaks to
        - Cross-disciplinary sourcing: Include Sociology, Economics, Psychology
      </capabilities>
      <presentation>
        - Never definitively declare a puzzle "solved"—let user judge
        - Surface relevant papers without auto-classification by subfield
        - Explicitly flag papers from outside Management (Sociology, Economics, Psychology)
      </presentation>
      <technical>
        - Aggressive caching to minimize API calls
        - Queue requests if rate limited
        - Show "searching literature..." with progress
      </technical>
    </literature_integration>

    <document_analysis>
      - Accept uploaded PDFs (draft papers, notes)
      - Evaluate existing framings against puzzle-quality criteria
      - Suggest strengthening weak articulations
    </document_analysis>

    <data_planning>
      - Help users articulate what data would NEED to address their puzzle
      - Essential for puzzle clarity—knowing what evidence would resolve it
    </data_planning>

    <output_generation>
      <puzzle_statement>
        - 1-2 paragraphs
        - Polished prose ready for paper introduction
      </puzzle_statement>
      <introduction_draft>
        - 2-3 pages
        - Generic academic style adaptable to any journal
      </introduction_draft>
      <research_brief>
        - Comprehensive document with 5 sections:
        - 1. The Puzzle
        - 2. Why It Matters
        - 3. Key Related Papers
        - 4. Evidence Needed
        - 5. Suggested Approach (methodology hints, data sources)
      </research_brief>
      <user_choice>User selects format at session end</user_choice>
    </output_generation>

    <session_management>
      <persistence>
        - No user accounts required
        - Session state preserved in browser during active use
        - Client-side state management
      </persistence>
      <export_options>
        - "Export conversation" (full session with all messages)
        - "Export outputs only" (artifacts only)
      </export_options>
      <export_formats>
        - JSON for re-import and session continuation
        - PDF for polished, human-readable outputs
      </export_formats>
      <import_behavior>
        - "Welcome back" summary before resuming
        - Full conversation context restored
      </import_behavior>
    </session_management>
  </core_features>

  <user_experience_flow>
    <welcome_screen>
      - "What brings you here today?"
      - Three entry mode cards: "I have an idea" | "I have data" | "I'm exploring"
      - Optional subfield selector (Strategy, OB, Entrepreneurship, etc.)
    </welcome_screen>

    <i_have_an_idea_flow>
      <phase_1_opening duration="0-5 minutes">
        - Opening prompt: "Tell me about the observation or pattern that sparked your interest. What have you noticed—in your data, in the field, or in your reading—that you find surprising or hard to explain?"
      </phase_1_opening>
      <phase_2_probing duration="5-20 minutes">
        - Follow-up questions to understand empirical pattern
        - Explore what existing theory would predict
        - Determine source (own data/fieldwork vs literature)
        - Pseudo-puzzle detection and gentle redirection if needed
      </phase_2_probing>
      <phase_3_literature duration="10-30 minutes">
        - Query Semantic Scholar for related work
        - Surface 3-5 closest papers
        - Ask "how is your angle different?"
        - Identify relevant theories and predictions
        - Flag cross-disciplinary sources
      </phase_3_literature>
      <phase_4_articulation duration="15-45 minutes">
        - Sharpen: What exactly is the empirical pattern?
        - Clarify: What does existing theory predict?
        - Establish: Why is the discrepancy non-trivial?
        - Define: What evidence would resolve it?
      </phase_4_articulation>
      <phase_5_output>
        - User requests preferred format
        - Generate polished output
        - Export session option
      </phase_5_output>
    </i_have_an_idea_flow>

    <i_have_data_flow>
      <step_1>Context first: "What is this data, and why did you collect it?"</step_1>
      <step_2>File upload and automatic summary generation</step_2>
      <step_3>Neutral presentation: "What stands out to you?"</step_3>
      <step_4>Anomaly exploration with rigor warnings</step_4>
      <step_5>Continues as "I have an idea" flow (literature, articulation, output)</step_5>
    </i_have_data_flow>

    <im_exploring_flow>
      <step_1>Broad questions: "What's been on your mind? What surprised you lately?"</step_1>
      <step_2>As themes emerge, query Semantic Scholar for debates</step_2>
      <step_3>Surface related papers and scholarly conversations</step_3>
      <step_4>Once direction emerges, continue as "I have an idea" flow</step_4>
    </im_exploring_flow>

    <conversation_interface>
      - Scrollable conversation history
      - User input area with send button
      - Action buttons: Upload File, Search Literature, Generate Output
      - Session controls: Export, Import
      - Settings: "Be direct" toggle, "Teach me" mode toggle
    </conversation_interface>
  </user_experience_flow>

  <api_endpoints_summary>
    <chat>
      - POST /api/chat - Main conversation endpoint with LLM
    </chat>
    <file_processing>
      - POST /api/upload - File upload and initial processing
      - POST /api/analyze - Run specific analysis on uploaded data
    </file_processing>
    <literature>
      - POST /api/literature - Semantic Scholar queries
    </literature>
    <output>
      - POST /api/generate - Generate puzzle statement, intro draft, or research brief
      - GET /api/export - Export session as JSON or PDF
      - POST /api/import - Import previous session
    </output>
  </api_endpoints_summary>

  <data_model>
    <session>
      - id: string (UUID)
      - mode: 'idea' | 'data' | 'exploring'
      - subfield: string (optional)
      - messages: Message[]
      - uploadedFiles: FileReference[]
      - analysisResults: AnalysisResult[]
      - literatureFindings: LiteratureResult[]
      - puzzleArtifacts: PuzzleArtifact[]
      - settings: { beDirectMode: boolean, teachMeMode: boolean }
      - createdAt: string (ISO timestamp)
      - lastModified: string (ISO timestamp)
    </session>
    <message>
      - role: 'user' | 'assistant' | 'system'
      - content: string
      - timestamp: string
      - metadata: { analysisTriggered, literatureQueried, feedbackLevel }
    </message>
    <puzzle_artifact>
      - type: 'statement' | 'introduction' | 'brief'
      - content: string
      - version: number
      - createdAt: string
    </puzzle_artifact>
    <analysis_result>
      - type: 'descriptive' | 'correlation' | 'anomaly' | 'subgroup' | 'theme' | 'quotes'
      - summary: string
      - details: object
      - rigorWarnings: string[]
    </analysis_result>
  </data_model>

  <ui_layout>
    <main_structure>
      - Full-width welcome screen with centered mode selection cards
      - After mode selection: Conversation-focused layout
      - Left: Main conversation area (70-80% width)
      - Right: Context panel showing uploaded files, analysis results, literature findings (collapsible)
      - Bottom toolbar: Action buttons and session controls
      - Top header: App title, subfield indicator, settings toggles
    </main_structure>
    <responsive_design>
      - Mobile: Single column, context panel as bottom sheet
      - Tablet: Collapsible side panel
      - Desktop: Full side-by-side layout
    </responsive_design>
  </ui_layout>

  <design_system>
    <color_palette>
      - Primary: Deep blue (academic, trustworthy)
      - Secondary: Warm amber (for highlights, calls to action)
      - Background: Clean white with subtle gray accents
      - Text: Dark charcoal for readability
      - Success: Green for completed artifacts
      - Warning: Orange for rigor warnings
      - Error: Red for validation errors
    </color_palette>
    <typography>
      - Headings: Inter or similar sans-serif
      - Body: System font stack for performance
      - Code/data: Monospace for variable names, statistics
    </typography>
    <tone>
      - Professional but approachable
      - Academic without being stuffy
      - Encouraging but rigorous
    </tone>
  </design_system>

  <error_handling>
    <file_upload>
      - Unsupported format: Clear error listing supported formats with conversion suggestions
      - Too large (>10MB): Explain limit, suggest sampling or splitting
      - Malformed file: Attempt multiple parsers, provide detailed error
    </file_upload>
    <api_errors>
      - Semantic Scholar rate limit: Queue requests, show progress, cache aggressively
      - LLM timeout: Retry with exponential backoff, offer to continue with cached context
    </api_errors>
    <user_input>
      - Minimal input: Prompt for more detail, explain what's needed
      - Persistent pseudo-puzzle: Acknowledge perspective, document framing with noted concerns, suggest human advisor
    </user_input>
    <data_analysis>
      - No anomalies found: Explain this is good news (data matches theory), suggest alternatives
      - Contradictory theories: Present as opportunity—this IS a puzzle
    </data_analysis>
    <session>
      - Browser crash: Encourage regular exports, offer recovery from last export
    </session>
  </error_handling>

  <performance_requirements>
    <targets>
      - Initial page load: less than 2 seconds
      - Chat response (simple): less than 3 seconds
      - Chat response (with analysis): less than 10 seconds
      - File upload + summary: less than 15 seconds for 10MB
      - Literature search: less than 5 seconds
      - Statistical analysis: less than 20 seconds
      - Export generation: less than 2 seconds
    </targets>
    <optimizations>
      - Use Claude Haiku for initial screening, Sonnet for deep analysis
      - Cache Semantic Scholar results aggressively
      - Batch API calls where possible
      - Lazy load analysis components
    </optimizations>
  </performance_requirements>

  <testing_strategy>
    <unit_tests>
      - Core logic for pseudo-puzzle detection
      - Analysis summarization functions
      - Session state management
      - Export/import serialization
    </unit_tests>
    <integration_tests>
      - API routes
      - File processing pipeline
      - LLM prompt handling
      - Semantic Scholar integration
    </integration_tests>
    <e2e_tests>
      - Full user flows through each entry mode
      - File upload and analysis flows
      - Export/import round-trip
      - Output generation
    </e2e_tests>
    <llm_output_testing>
      - Evaluation suite for response quality
      - Puzzle rigor assessment
      - Feedback appropriateness
      - Pseudo-puzzle detection accuracy
    </llm_output_testing>
    <load_testing>
      - Verify performance under concurrent users
    </load_testing>
  </testing_strategy>

  <implementation_steps>
    <step number="1">
      <title>Project Setup and Core Infrastructure</title>
      <tasks>
        - Initialize Next.js project with TypeScript
        - Configure Tailwind CSS and shadcn/ui
        - Set up project structure and routing
        - Create Python FastAPI service skeleton
        - Configure environment variables and API key handling
      </tasks>
    </step>
    <step number="2">
      <title>Welcome Screen and Entry Mode Selection</title>
      <tasks>
        - Build welcome screen layout
        - Create entry mode selection cards
        - Implement subfield selector dropdown
        - Set up client-side session state management
        - Create navigation flow to conversation interface
      </tasks>
    </step>
    <step number="3">
      <title>Conversation Interface Foundation</title>
      <tasks>
        - Build chat message components
        - Implement message history with scrolling
        - Create user input area with send functionality
        - Add loading states and typing indicators
        - Implement basic session state persistence
      </tasks>
    </step>
    <step number="4">
      <title>LLM Integration and Core Dialogue</title>
      <tasks>
        - Create /api/chat endpoint
        - Implement Claude API integration
        - Build system prompts for puzzle-quality assessment
        - Implement conversation context management
        - Add feedback level tracking
      </tasks>
    </step>
    <step number="5">
      <title>Pseudo-Puzzle Detection System</title>
      <tasks>
        - Implement pattern detection for gap-spotting, black-box, overlooked
        - Build graduated feedback escalation logic
        - Create redirection suggestions
        - Implement "Be direct" mode toggle
        - Build "Teach Me" mode with educational content
      </tasks>
    </step>
    <step number="6">
      <title>File Upload and Processing</title>
      <tasks>
        - Create /api/upload endpoint
        - Implement file format detection and validation
        - Build CSV/Excel parsing
        - Integrate Python service for Stata/SPSS/R files
        - Create file summary generation
      </tasks>
    </step>
    <step number="7">
      <title>Quantitative Data Analysis</title>
      <tasks>
        - Build descriptive statistics module in Python service
        - Implement pattern detection algorithms
        - Create anomaly detection with rigor warnings
        - Build subgroup analysis
        - Implement multiple testing warnings
      </tasks>
    </step>
    <step number="8">
      <title>Qualitative Data Analysis</title>
      <tasks>
        - Implement theme detection using LLM
        - Build surprising quote surfacing
        - Create document summarization
        - Integrate qualitative results into conversation
      </tasks>
    </step>
    <step number="9">
      <title>Literature Integration</title>
      <tasks>
        - Create /api/literature endpoint
        - Implement Semantic Scholar API integration
        - Build novelty checking (surface similar papers)
        - Implement theory extraction from search results
        - Add cross-disciplinary source flagging
        - Implement caching and rate limit handling
      </tasks>
    </step>
    <step number="10">
      <title>Document Analysis</title>
      <tasks>
        - Implement PDF text extraction
        - Build framing evaluation against puzzle criteria
        - Create strengthening suggestions
        - Integrate document context into conversation
      </tasks>
    </step>
    <step number="11">
      <title>Output Generation</title>
      <tasks>
        - Create /api/generate endpoint
        - Build puzzle statement generator
        - Implement introduction draft generator
        - Create research brief generator (5 sections)
        - Add output preview and editing
      </tasks>
    </step>
    <step number="12">
      <title>Session Export and Import</title>
      <tasks>
        - Implement JSON export (full session)
        - Create PDF export for polished outputs
        - Build import functionality with validation
        - Create "Welcome back" summary on import
        - Add export encryption option
      </tasks>
    </step>
    <step number="13">
      <title>Context Panel and UI Polish</title>
      <tasks>
        - Build collapsible context panel
        - Display uploaded files, analysis results, literature
        - Implement responsive design (mobile, tablet, desktop)
        - Add loading states and progress indicators
        - Ensure accessibility (keyboard nav, screen readers)
      </tasks>
    </step>
    <step number="14">
      <title>Error Handling and Edge Cases</title>
      <tasks>
        - Implement comprehensive error handling
        - Add user-friendly error messages
        - Build retry logic for API failures
        - Handle browser crash recovery
        - Add input validation throughout
      </tasks>
    </step>
    <step number="15">
      <title>Testing and Quality Assurance</title>
      <tasks>
        - Write unit tests for core logic
        - Create integration tests for API routes
        - Build E2E tests for user flows
        - Implement LLM output evaluation suite
        - Perform load testing
      </tasks>
    </step>
    <step number="16">
      <title>Performance Optimization and Deployment</title>
      <tasks>
        - Optimize bundle size and loading
        - Implement caching strategies
        - Configure Vercel deployment
        - Deploy Python service to Railway
        - Set up monitoring and error tracking
        - Final production hardening
      </tasks>
    </step>
  </implementation_steps>

  <success_criteria>
    <functionality>
      - All three entry modes fully operational
      - Pseudo-puzzle detection working with graduated feedback
      - File upload supporting all specified formats
      - Quantitative and qualitative analysis producing meaningful insights
      - Literature integration surfacing relevant papers and theories
      - All three output formats generating polished, usable content
      - Session export/import working seamlessly
    </functionality>
    <user_experience>
      - Intuitive flow requiring no external documentation
      - Conversation feels natural and academically rigorous
      - Feedback is helpful, not discouraging
      - Clear progress indication throughout session
      - Mobile and desktop experiences both polished
    </user_experience>
    <technical_quality>
      - All performance targets met
      - Robust error handling with graceful degradation
      - Secure handling of user data (transient only)
      - Comprehensive test coverage
      - Clean, maintainable codebase
    </technical_quality>
    <design_polish>
      - Professional, academic aesthetic
      - Consistent visual language throughout
      - Accessible to users with disabilities
      - Responsive across all device sizes
    </design_polish>
  </success_criteria>

  <out_of_scope>
    - Journal-specific advice or venue recommendations
    - Explicit teaching of Zuckerman's 10 genres framework (unless Teach Me mode)
    - Data storage beyond session (users export their own work)
    - Large dataset handling (>10MB)
    - Full research design/methodology planning
    - User accounts and authentication
    - Collaborative real-time features (v1)
  </out_of_scope>

  <future_considerations>
    - Multiple LLM backend options
    - Collaborative features (share puzzles with co-authors)
    - Version history within a session
    - Integration with reference managers (Zotero, Mendeley)
    - On-premises deployment option for institutions
  </future_considerations>
</project_specification>
